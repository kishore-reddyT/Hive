{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\").set(\"spark.cores.max\", \"4\"))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 4).map(parse_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\n",
      "Anarchism\n",
      "is\n",
      "often\n",
      "defined\n",
      "as\n",
      "a\n",
      "political\n",
      "philosophy\n",
      "which\n",
      "holds\n",
      "the\n",
      "state\n",
      "to\n",
      "be\n",
      "undesirable\n",
      "unnecessary\n",
      "or\n",
      "harmful\n",
      "The\n",
      "following\n",
      "sources\n",
      "cite\n",
      "anarchism\n",
      "as\n",
      "a\n",
      "political\n",
      "philosophy\n",
      "Slevin\n",
      "Carl\n",
      "Anarchism\n",
      "The\n",
      "Concise\n",
      "Oxford\n",
      "Dictionary\n",
      "of\n",
      "Politics\n",
      "Ed\n",
      "Iain\n",
      "McLean\n",
      "and\n",
      "Alistair\n",
      "McMillan\n",
      "Oxford\n",
      "University\n",
      "Press\n",
      "2003\n",
      "However\n",
      "others\n",
      "argue\n"
     ]
    }
   ],
   "source": [
    "result = wiki.take(1)[0]\n",
    "for word in result[:50]:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pairs\n",
    "### Intro\n",
    "In this assignment you will use Spark to compute various statistics for word pairs. At the same time, you will learn some simple techniques of natural language processing.\n",
    "\n",
    "Dataset location: /data/wiki/en_articles\n",
    "\n",
    "Format: article_id <tab> article_text\n",
    "\n",
    "While parsing the articles, do not forget about Unicode (even though this is an English Wikipedia dump, there are many characters from other languages), remove punctuation marks and transform words to lowercase to get the correct quantities.\n",
    "\n",
    "### Task\n",
    "Find all the pairs of two consequent words where the first word is “narodnaya”. For each pair, count the number of occurrences in the Wikipedia dump. Print all the pairs with their count in a lexicographical order. Output format is “word_pair <tab> count”, for example:\n",
    "\n",
    "```\n",
    "red_apple\t100500\n",
    "\n",
    "crazy_zoo\t42\n",
    "```\n",
    "\n",
    "Note that two words in a pair are concatenated with the underscore character, and the result is in the lowercase.\n",
    "\n",
    "One motivation for counting these continuations is to get a better understanding of the language. Some words, like “the”, have a lot of continuations, while others, like “San”, have just a few (“San Francisco”, for example). One can build a language model with these statistics. If you are interested to learn more, search for “n-gram language model” in the Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_starting_from_word(words, first_word='word'):\n",
    "    pairs = []\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        if (word == first_word):\n",
    "            pair = '{}_{}'.format(word, words[i+1])\n",
    "            cnt = 1\n",
    "            pairs.append((pair, cnt))\n",
    "        else:\n",
    "            continue\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words\n",
    "wiki_lower = wiki.map(lambda words: [x.lower() for x in words])\n",
    "\n",
    "# find pairs starting from defined word\n",
    "wiki_pairs = wiki_lower.flatMap(lambda x: pairs_starting_from_word(x, 'narodnaya'))\n",
    "\n",
    "# filtering empty elements\n",
    "wiki_pairs = wiki_pairs.filter(lambda x: x != [])\n",
    "\n",
    "# aggregate counters\n",
    "wiki_red = wiki_pairs.reduceByKey(lambda a, b: a + b, numPartitions=16)\n",
    "\n",
    "# sort values by key\n",
    "wiki_red_sorted = wiki_red.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narodnaya_gazeta\t1\n",
      "narodnaya_volya\t9\n"
     ]
    }
   ],
   "source": [
    "result = wiki_red_sorted.collect()\n",
    "for pair, cnt in result:\n",
    "    print '{}\\t{}'.format(pair, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "### Intro\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "To build an intuition behind the definition, consider the following cases:\n",
    "\n",
    "- ***“roman empire”;*** assume that this is a unique combination, and every occurrence of “roman” is followed by “empire”, and, vice versa, every occurrence of “empire” is preceded by “roman”. In this case, “P(ab) = P(a) = P(b)”, so “PMI(a, b) = -ln P(a) = -ln P(b)”. This quantity increases when the probability of the collocation is low.\n",
    "\n",
    "- ***“the doors”;*** let’s assume that “the” may occur with every word, independently. Thus, “P(ab) = P(a)*P(b)”, and “PMI(a, b) = ln 1 = 0”.\n",
    "\n",
    "- ***“green idea / sleeps furiously”;*** when two words never occur together, “P(ab) = 0”, and “PMI(a, b) = -inf”. Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "### Task\n",
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, “high school” or “roman empire”.\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”, where P(ab) is the probability of two words coming one after the other, and P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is “P(a) = # of occurrences of word a / total number of words”, and “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as “NPMI(a, b) = PMI(a, b) / -ln P(ab)”. This normalizes the quantity to be within the range [-1; 1].\n",
    "\n",
    "You task is a bit more complicated now:\n",
    "\n",
    "- Extract all the words, as in the previous task.\n",
    "- Filter out stopwords using the dictionary (/datasets/stop_words_en.txt ) (do not forget to convert words to the lowercase!)\n",
    "- Compute all bigrams (that is, pairs of consequent words)\n",
    "- Leave only bigrams with at least 500 occurrences\n",
    "- Compute NPMI for every bigram (note: when computing probabilities, you need unpruned counts!)\n",
    "- Sort word pairs by NPMI in the descending order\n",
    "- Print top 39 word pairs, with words delimited by the underscore “_”\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "roman_empire\n",
    "\n",
    "south_africa\n",
    "```\n",
    "\n",
    "***Hint:*** if you did everything right, “roman_empire” and “south_africa” are going to be in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "with open('/datasets/stop_words_en.txt', 'r') as f:\n",
    "    stop_words = set(f.read().split())\n",
    "\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bill',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'co',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fify',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herse\"',\n",
       " 'him',\n",
       " 'himse\"',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'interest',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itse\"',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mill',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myse\"',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'same',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sincere',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'system',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams(words):\n",
    "    bigrams = []\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        pair = u'_'.join((word, words[i+1])).encode('utf-8')\n",
    "        cnt = 1\n",
    "        bigrams.append((pair, cnt))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def calc_npmi(pair, cnt, words_occurrences_dict, total_num_of_words, total_num_of_pairs):\n",
    "    word1, word2 = pair.split('_')\n",
    "    p_a = words_occurrences_dict[word1] / total_num_of_words\n",
    "    p_b = words_occurrences_dict[word2] / total_num_of_words\n",
    "    \n",
    "    pmi_ab = cnt / total_num_of_pairs\n",
    "    pmi_a_b = log(pmi_ab / (p_a * p_b))\n",
    "    \n",
    "    nmpi_a_b = pmi_a_b / -log(pmi_ab)\n",
    "    return (pair, nmpi_a_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words\n",
    "wiki_lower = wiki.map(lambda words: [x.lower() for x in words])\n",
    "\n",
    "# words not in stop_words_en.txt\n",
    "wiki_filt = wiki_lower.map(lambda words: [x for x in words if x not in stop_words])\n",
    "\n",
    "# create bigrams\n",
    "wiki_bigrams = wiki_filt.flatMap(create_bigrams)\n",
    "\n",
    "# aggregate counters\n",
    "wiki_red = wiki_bigrams.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# filter values by counter\n",
    "wiki_red_filt = wiki_red.filter(lambda (pair, cnt): cnt >= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6971026"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of words\n",
    "tot_num_words = wiki_filt.map(lambda words: len(words))\n",
    "tot_num_words = tot_num_words.reduce(lambda a, b: a + b)\n",
    "tot_num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6966926"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of words pairs\n",
    "tot_num_pairs = wiki_filt.map(lambda words: len(words) - 1)\n",
    "tot_num_pairs = tot_num_pairs.reduce(lambda a, b: a + b)\n",
    "tot_num_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of each word occurrences\n",
    "words_occ = wiki_filt.flatMap(lambda words: [(x, 1) for x in words])\n",
    "words_occ = words_occ.reduceByKey(lambda a, b: a + b)\n",
    "words_occ = words_occ.filter(lambda (pair, cnt): cnt >= 500)\n",
    "words_occ = words_occ.collect()\n",
    "\n",
    "words_occ_dict = dict()\n",
    "for item, cnt in words_occ:\n",
    "    words_occ_dict[item] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_npmi = wiki_red_filt\\\n",
    "    .map(lambda (pair, cnt): calc_npmi(pair, cnt, words_occ_dict, tot_num_words, tot_num_pairs))\\\n",
    "    .map(lambda (a, b): (b, a))\\\n",
    "    .sortByKey(False)\\\n",
    "    .map(lambda (a, b): (b, a))\\\n",
    "    .take(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs_npmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los_angeles\n",
      "external_links\n",
      "united_states\n",
      "prime_minister\n",
      "san_francisco\n",
      "et_al\n",
      "new_york\n",
      "supreme_court\n",
      "19th_century\n",
      "20th_century\n",
      "references_external\n",
      "soviet_union\n",
      "air_force\n",
      "baseball_player\n",
      "university_press\n",
      "roman_catholic\n",
      "united_kingdom\n",
      "references_reading\n",
      "notes_references\n",
      "award_best\n",
      "north_america\n",
      "new_zealand\n",
      "civil_war\n",
      "catholic_church\n",
      "world_war\n",
      "war_ii\n",
      "south_africa\n",
      "took_place\n",
      "roman_empire\n",
      "united_nations\n",
      "american_singer-songwriter\n",
      "high_school\n",
      "american_actor\n",
      "american_actress\n",
      "american_baseball\n",
      "york_city\n",
      "american_football\n",
      "years_later\n",
      "north_american\n"
     ]
    }
   ],
   "source": [
    "for pair, npmi in pairs_npmi:\n",
    "    print pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
